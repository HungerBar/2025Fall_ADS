
\documentclass[13pt,hyperref,a4paper,UTF8]{ctexart}
\usepackage{zjuEnreport}
\usepackage{booktabs}

\renewcommand{\figurename}{Figure}

% 自定义表格前缀（如将“表”改为“Table”）
\renewcommand{\tablename}{Table}
\renewcommand{\contentsname}{Contents}
\renewcommand{\refname}{Reference}
%%-------------------------------正文开始---------------------------%%
\begin{document}

%%-----------------------封面--------------------%%
\cover
\thispagestyle{empty}
%%------------------摘要-------------%%
\newpage
%\begin{abstract}
%\end{abstract}

%%--------------------------目录页------------------------%%
\tableofcontents

%%------------------------正文页从这里开始-------------------%
\newpage

%%可选择这里也放一个标题
\begin{center}
    \title{ \Huge \textbf{{}}}
\end{center}

\section{Project Description}
This project aims to solve the \textbf{3-Partition Problem}. Given a multiset $S = \{a_1, a_2, \dots, a_N\}$ containing $N$ positive integers, we need to determine whether $S$ can be partitioned into three disjoint subsets $S_1, S_2, S_3$ such that the sum of the elements in each subset is equal.

Formally, let the total sum be $Sum = \sum_{i=1}^N a_i$. We need to find a partition that satisfies:
\begin{equation}
    \sum_{x \in S_1} x = \sum_{y \in S_2} y = \sum_{z \in S_3} z = \text{Target}
\end{equation}
where $\text{Target} = Sum / 3$. If $Sum$ is not divisible by 3, it is immediately determined that no solution exists. If such a partition exists, the algorithm must output the specific subset partition scheme; otherwise, it outputs ``no''.

\section{Description of Algorithms}
To address this problem, we designed and implemented three algorithms: Depth-First Search (DFS) with pruning, pseudo-polynomial time Dynamic Programming (DP), and heuristic Simulated Annealing (SA).

\subsection{Algorithm 1: Depth-First Search (DFS) with Pruning}
DFS is an exact algorithm that traverses all possible states by recursively building a search tree. The core of the algorithm is a recursive function \texttt{DFS(index, bucket\_sums)}, where \texttt{index} represents the index of the number currently being processed, and \texttt{bucket\_sums} maintains the cumulative sums of the three buckets. To handle the worst-case exponential search space of $O(3^N)$, we employed two pruning strategies:

\begin{enumerate}
    \item \textbf{Sort Descending}: 
    Before the search begins, we sort the input array $S$ in descending order.
    \begin{itemize}
        \item \textit{Principle}: Prioritizing larger values can reduce the remaining capacity of buckets more quickly. Larger elements have less flexibility (harder to fit into buckets with small remaining space), so they can trigger capacity overflow conditions earlier, achieving ``Fail-Fast'' and significantly reducing the depth of invalid recursion.
    \end{itemize}
    
    \item \textbf{Symmetry Breaking}: 
    This is the core optimization of the algorithm, used to eliminate isomorphic subtrees in the search tree.
    \begin{itemize}
        \item \textit{Principle}: Initially, the three empty buckets are mathematically completely equivalent (i.e., the partition $\{A, B, C\}$ is the same solution as $\{B, A, C\}$). If putting the current number $a_{index}$ into the first empty bucket leads to a subsequent search failure, then attempting to put it into the second or third empty bucket will inevitably yield the same result.
        \item \textit{Implementation}: During the recursion, if the current bucket is empty (Sum=0) and placing the current number into this bucket results in backtracking (meaning this branch has no solution), we strictly stop attempting any subsequent empty buckets. This effectively defines a ``filling order'' for the buckets, reducing the search space by approximately $3!$.
    \end{itemize}
\end{enumerate}

\subsection{Algorithm 2: Dynamic Programming (DP)}
We model this problem as a variant of the multi-dimensional 0/1 Knapsack problem. Since we need to distribute numbers precisely into three buckets, we only need to track the states of the first two buckets; the state of the third bucket is implicitly determined by the law of conservation of the sum.

\begin{itemize}
    \item \textbf{State Definition}: 
    Let $dp[i][j]$ be a boolean value indicating whether it is possible to select a subset of items from the first $k$ items to fill Bucket 1 to capacity $i$ and Bucket 2 to capacity $j$. If $dp[i][j]$ is true, and $TotalSum\_k - i - j \le Target$, then the state is valid.
    
    \item \textbf{State Transition Equation}: 
    For the $k$-th item (with weight $v$), we traverse all possible $(i, j)$ states. The new state $dp_{new}$ can be derived from the previous state $dp_{old}$:
    \begin{equation}
        dp[i][j] = \underbrace{dp[i][j]}_{\text{Put in Bucket 3}} \lor \underbrace{dp[i-v][j]}_{\text{Put in Bucket 1}} \lor \underbrace{dp[i][j-v]}_{\text{Put in Bucket 2}}
    \end{equation}
    The boundary condition is $dp[0][0] = \text{True}$.
    
    \item \textbf{Path Recording and Backtracking}: 
    To output the specific scheme, we maintain a 3D array $path[k][i][j] \in \{1, 2, 3\}$.
    \begin{itemize}
        \item When we place the $k$-th item into Bucket 1 and update state $(i, j)$, we record $path[k][i][j] = 1$.
        \item After the algorithm finishes, if $dp[Target][Target]$ is true, we backtrack from $(N, Target, Target)$ to reconstruct the assignment of each item based on the $path$ array.
    \end{itemize}
\end{itemize}

\subsection{Algorithm 3: Simulated Annealing (SA)}
To solve for large-scale inputs ($N=1000$) or the high-dimensional partition problems in the Bonus section (where Exact Algorithms would fail due to time or memory exhaustion), we implemented Simulated Annealing, a probability-based heuristic algorithm.

\begin{enumerate}
    \item \textbf{Initialization}: 
    Randomly assign $N$ numbers to 3 buckets to form the initial solution $S_0$.
    
    \item \textbf{Cost Function (Energy)}: 
    We define the system's ``energy'' as the degree of imbalance in the current state. The goal is to find the global minimum energy $E=0$.
    \begin{equation}
        E(S) = \sum_{m=1}^{3} |Sum_m - \text{Target}|
    \end{equation}
    
    \item \textbf{Neighbor Generation}: 
    In each iteration, we generate a new state $S'$ by perturbing the current state. Two strategies are adopted:
    \begin{itemize}
        \item \textit{Move}: Randomly select a number and move it from its current bucket to another random bucket.
        \item \textit{Swap}: Randomly select two numbers located in different buckets and swap their positions.
    \end{itemize}
    
    \item \textbf{Metropolis Criterion}: 
    Let $\Delta E = E(S') - E(S)$.
    \begin{itemize}
        \item If $\Delta E < 0$ (the new state is better), accept the new state unconditionally.
        \item If $\Delta E \ge 0$ (the new state is worse), accept the new state with probability $P = \exp(-\Delta E / T)$, where $T$ is the current temperature. This mechanism allows the algorithm to climb ``uphill'' in the early stages to escape local optima.
    \end{itemize}
    
    \item \textbf{Time-Limited Restart}:
    Since SA is a probabilistic algorithm, a single run may not converge. We introduced a restart mechanism: if no solution is found ($E > 0$) when cooling ends and the total runtime has not exceeded 0.9 seconds, we reset the temperature and random seed to start a new round of annealing. This maximizes the probability of finding a solution within a limited time.
\end{enumerate}

\section{Theoretical Analysis of Time Complexity}

\subsection{Problem Complexity Classification}
\begin{itemize}
    \item The \textbf{3-Partition Problem} is a \textbf{Strongly NP-Complete} problem.
    \item Strong NP-completeness implies that the problem remains NP-Complete even if the magnitude (Value) of the input numbers is bounded by a polynomial. This also implies that unless $P=NP$, there is no \textbf{Fully Polynomial Time Approximation Scheme} for this problem.
    \item \textbf{Conclusion}: There is no exact algorithm with polynomial time complexity regarding the bit length of the input data.
\end{itemize}

\subsection{Algorithm 1: Depth-First Search (DFS)}
\begin{itemize}
    \item \textbf{Worst-Case Time Complexity}: $O(3^N)$
    \item \textbf{Detailed Analysis}: 
    The DFS algorithm essentially traverses a ternary tree. For $N$ elements in the set, each element has 3 possible destinations (Bucket 1, Bucket 2, or Bucket 3). Therefore, the size of the unpruned search space is $3 \times 3 \times \dots \times 3 = 3^N$.
    \item \textbf{Impact of Pruning}: 
    Although we implemented \textit{Sort Descending} and \textit{Symmetry Breaking}, which greatly prune the search tree for randomly generated data (making the average branching factor far less than 3), in the worst case (e.g., carefully constructed adversarial data, or when all numbers are equal and there is no solution), pruning may fail, and the algorithm still needs to explore most of the state space. Thus, its theoretical upper bound remains exponential.
\end{itemize}

\subsection{Algorithm 2: Dynamic Programming (DP)}
\begin{itemize}
    \item \textbf{Time Complexity}: $O(N \cdot \text{Target}^2)$, where $\text{Target} = \text{Sum}/3$.
    \item \textbf{Space Complexity}: $O(N \cdot \text{Target}^2)$.
    \item \textbf{Pseudo-polynomial Time}: 
    The formula contains polynomials of $N$ and $\text{Target}$, which seems efficient. However, in computational complexity theory, the time complexity of an algorithm is relative to the input size, which is calculated based on the binary bit length of the data.
    \begin{itemize}
        \item Assuming the maximum value of input numbers is $M$, the number of bits required to input this number is $L = \log_2 M$.
        \item The running time of the algorithm is proportional to the value $M$ (i.e., $\text{Target}$), which means it is proportional to $2^L$.
        \item Therefore, the running time grows \textbf{exponentially} relative to the input size $L$.
    \end{itemize}
    \textbf{Conclusion}: When $N$ is large but the numeric values $M$ are small (e.g., $M \le N$), DP is an effective polynomial-time solution; however, when the numeric values $M$ are large (e.g., $M = 10^{100}$), DP will fail due to memory exhaustion or timeout.
\end{itemize}

\subsection{Algorithm 3: Simulated Annealing (SA)}
\begin{itemize}
    \item \textbf{Time Complexity}: $O(K_{iter} \cdot T_{update})$
    \item \textbf{Analysis}: 
    The complexity of Simulated Annealing is not directly determined by the input size $N$, but by the user-defined cooling schedule.
    \begin{itemize}
        \item $K_{iter}$: Total number of iterations, approximately $\log_{\alpha}(\frac{T_{end}}{T_{start}})$, where $\alpha$ is the cooling coefficient.
        \item $T_{update}$: The cost of each state transition (move or swap) and energy function update. In an optimized implementation, we can complete the energy update in $O(1)$ time via Incremental Update.
    \end{itemize}
    Therefore, the total complexity mainly depends on the preset parameters. In our implementation, we added a hard time limit (0.9 seconds). This makes the algorithm exhibit $O(1)$ constant time complexity in an engineering sense (it always stops within a fixed time relative to infinite problem growth), but at the cost of being unable to guarantee the completeness of the solution or prove the non-existence of a solution.
\end{itemize}

\section{实验与测试分析}

本实验对三种算法方法（DFS+剪枝、动态规划（DP）、模拟退火（SA））在三分类问题上的性能进行了系统测试，测试设计考虑了DFS在小规模输入上的表现, 剪枝策略的优化, 输入规模 $n$ 与总和 $sum$ 的变化对DP算法性能的影响, 以及 SA 算法在大规模输入上的稳定性

\subsection{测试数据设计}

本实验共设计三类测试数据，分别用于评估 DFS、DP 与模拟退火（SA）算法在不同规模与结构下的性能表现。

首先，在 DFS 部分，我们设计了两类测试数据：一类是小规模 $n$ 但具有较大 $sum$ 的输入，用于考察剪枝策略在极端情况下的有效性；另一类是 $n$ 与 $sum$ 均较小的典型输入，用于基准测试 DFS 在常规规模下的运行性能。

对于 DP 算法，我们构造了三组测试数据。第一组固定 $n$ 并逐步增大 $sum$，用于观察 DP 算法在状态空间随总和增大时的时间增长趋势。第二组固定 $sum$ 但逐步增大 $n$，以分析 DP 对输入规模的敏感性。第三组则同时连续增加 $n$ 与 $sum$，用于评估 DP 在大规模数据下的可计算范围，并对其经验时间复杂度进行拟合分析。

最后，在模拟退火（SA）部分，为了构造既能保证可解性、又能使 DP 难以处理的大规模测试实例(符合 SA 的实际使用情况)，我们采用两种思路(base construction 和 greedy fill-in)构造出了n 和sum都较大的测试集, 并让 \texttt{/sa} 在至多重启 10 次的情况下观察性能

\subsection{DFS + 剪枝测试结果}

在小规模 $n$，较大 $sum$ 的数据下，DFS 算法在剪枝策略失效时，计算时间显著增加，部分数据甚至出现计算时间过长的现象。测试结果统计如表~\ref{tab:dfs_time_stats}

\begin{table}
\centering
\caption{DFS 算法运行时间统计（秒）}
\begin{tabular}{c|c|c|c|c}
\hline
$n$ & 平均时间 & 最大时间 & 最小时间 & 方差 \\
\hline
10 & 0.0080 & 0.0182 & 0.0054 & 5.12e-05 \\
15 & 0.0081 & 0.0145 & 0.0044 & 2.19e-05 \\
20 & 0.0078 & 0.0149 & 0.0042 & 1.92e-05 \\
25 & 0.0064 & 0.0101 & 0.0043 & 6.82e-06 \\
30 & 0.0921 & 0.1373 & 0.0049 & 0.0045 \\
35 & 2.3341 & 9.2076 & 0.0043 & 7.41 \\
40 & 3.1284 & 11.2849 & 0.0043 & 17.92 \\
45 & --- & 超时 & 0.0040 & --- \\
50 & 103.034 & 254.024 & 0.0044 & 12813.5 \\
\hline
\end{tabular}
\label{tab:dfs_time_stats}
\end{table}

\begin{itemize}
    \item 当剪枝条件失效时，DFS 的搜索空间呈指数级增长，时间复杂度接近 $O(k^n)$，其中 $k$ 为每个元素可选分类数量。
    \item $n=45$ 时出现超时现象，表明剪枝失效对 DFS 的影响极大。
    \item 对于小规模 $n<30$，剪枝有效时算法运行时间在毫秒级，性能较好。
\end{itemize}

\subsection{动态规划（DP）测试结果}

为了系统评估 DP 算法在三分类问题上的性能，我们分别在以下三种典型场景下进行了实验：
（1）固定 $n$、增大 $sum$；（2）固定 $sum$、增大 $n$；（3）同时逐步提高 $n$ 与 $sum$。
以下对三类实验结果分别进行分析。


\subsubsection{固定 $n$，增大 $sum$ 的情况}

在该实验中，我们固定元素数量 $n$，逐步增加总和 $sum$，并记录 DP 算法的运行时间。实验结果如图~\ref{fig:dp_fix_n} 所示。可以明显观察到，随着 $sum$ 的增大，DP 的运行时间呈现非线性增长趋势，但总体与 DP 的理论复杂度 $O(n \cdot sum)$ 保持一致。

\begin{table}[htbp]
\centering
\caption{固定 $n$，增大 $sum$ 时 DP 算法运行时间统计（秒）}
\label{tab:dp_fix_n}
\begin{tabular}{cccccc}
\hline
$total\_sum$ & mean & min & max & var \\
\hline
10002  & 0.3868 & 0.3765 & 0.4156 & 0.000266 \\
20001  & 1.5310 & 1.4883 & 1.6145 & 0.003158 \\
30000  & 3.3737 & 3.3543 & 3.4249 & 0.000853 \\
40002  & 6.4288 & 6.3932 & 6.4482 & 0.000509 \\
50001  & 11.5226 & 11.3661 & 11.6768 & 0.012789 \\
60000  & 18.0380 & 16.6423 & 23.2785 & 8.589885 \\
70002  & 24.0130 & 22.8261 & 26.7825 & 3.014150 \\
80001  & 30.6717 & 29.9962 & 32.8286 & 1.466136 \\
90000  & 37.7163 & 37.1789 & 39.1276 & 0.661494 \\
100002 & 45.1194 & 43.9791 & 48.8319 & 4.401916 \\
\hline
\end{tabular}
\end{table}

从数据中可以看出：

\begin{itemize}
    \item 当 $sum$ 较小（例如 $10^4$ 以下）时，运行时间增长相对平缓，均值维持在 $O(0.3\sim0.4)$ 秒的水平。
    \item 随着 $sum$ 扩展到 $10^5$ 量级，运行时间呈明显加速增长，尤其是方差也随之增大，显示状态表规模显著增加对性能的影响。
    \item 对不同复杂度假设的拟合结果显示，$O(n^2)$ 拟合曲线与实验数据最为贴合（MSE = 0.772），拟合参数为 $[4.662\times10^{-9}, -0.0684]$，远优于 $O(n)$（MSE = 9.41）或 $O(n \log n)$（MSE = 7.45）的拟合效果。
\end{itemize}

实验数据及拟合曲线见图~\ref{fig:dp_fix_n}。

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figures/fixedn.png} % 将 your_image_file.png 替换为实际图片文件名
\caption{固定 $n$，增大 $sum$ 时 DP 算法运行时间及不同复杂度拟合曲线对比}
\label{fig:dp_fix_n}
\end{figure}

综上，固定 $n$ 条件下，DP 算法运行时间随 $sum$ 单调增加，趋势稳定且可预测；即便在大规模 $sum$ 条件下，其性能仍明显优于 DFS，表现出良好的稳定性和可扩展性。
\subsubsection{固定 $sum$，增大 $n$ 的情况}

本组实验固定总和 $sum$，逐步增加输入规模 $n$，测试结果如图~\ref{fig:dp_fix_sum} 所示。DP 的状态空间大小与 $n$ 成线性关系，因此运行时间的增长主要由迭代次数增加所引起。

\begin{table}[htbp]
\centering
\caption{固定 $sum$，增大 $n$ 时 DP 算法运行时间统计（秒）}
\label{tab:dp_fix_sum}
\begin{tabular}{cccccc}
\hline
$n$ & mean & min & max & var \\
\hline
100  & 4.3085  & 4.2870  & 4.3247  & 0.000170 \\
200  & 10.1568 & 9.9115  & 10.7473 & 0.090331 \\
300  & 19.5679 & 15.8822 & 27.2218 & 16.318133 \\
400  & 22.3316 & 21.8123 & 23.3965 & 0.314617 \\
500  & 30.9510 & 28.7956 & 34.6313 & 4.063425 \\
600  & 36.7893 & 35.6425 & 38.6712 & 1.007763 \\
700  & 47.5941 & 43.3565 & 50.0603 & 5.547414 \\
800  & 56.2361 & 49.5285 & 66.9230 & 38.305580 \\
900  & 62.0149 & 58.8609 & 68.4820 & 12.228341 \\
1000 & 68.6487 & 65.5686 & 76.3955 & 16.920649 \\
\hline
\end{tabular}
\end{table}

拟合曲线~\ref{fig:dp_fix_sum}中可以观察到：

\begin{itemize}
    \item 在中小规模 $n$ 范围内，DP 的运行时间基本保持线性增长，拟合结果表明 $O(n)$ 拟合较为贴合。通过最小二乘法拟合得到的时间复杂度函数为：
    
    \begin{equation}
    T(n) = 0.073154 \cdot n - 4.374624
    \end{equation}
    
    拟合的均方误差（MSE）为 $12.146$ 仍然可以接受
\end{itemize}


\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figures/dp_time_fit_en.png}
\caption{固定 $sum$，增大 $n$ 时 DP 算法运行时间及 O(n) 拟合曲线}
\label{fig:dp_fix_sum}
\end{figure}
\subsubsection{同时增大 $n$ 与 $sum$ 的情况}

为了进一步评估动态规划算法在大规模输入下的极限性能，我们系统地同时增大输入规模 $n$（项目数量）与 $sum$（目标和），使DP的状态空间呈二次方级增长（$O(n \cdot sum^2)$）。测试结果如图~\ref{fig:dp_scale_up}所示。

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/3D.png}
\caption{动态规划算法在$n$与$sum$同时增大时的性能可视化。图中蓝色散点为实际运行时间，绿色曲面为基于$O(n \cdot sum^2)$复杂度模型的拟合结果。参数拟合结果为$T = 2.07 \times 10^{-10} \cdot n \cdot sum^2 - 0.179$，拟合优度$R^2 = 0.9959$，表明模型能够准确描述算法的时间复杂度。}
\label{fig:dp_scale_up}
\end{figure}

从可视化结果可以清晰观察到，当$n$与$sum$同时扩大时，DP算法的运行时间呈快速增长趋势。具体而言：

\begin{itemize}
    \item \textbf{指数级增长特征}：运行时间随$n$和$sum$的增大呈现明显的非线性增长，与$O(n \cdot sum^2)$的理论复杂度高度一致（$R^2 = 0.9959$）。
    
    \item \textbf{硬件限制触及}：在$n \in [50, 450]$，$sum \in [5208, 47349]$的测试范围内，最大运行时间已显著增长，并在部分数据点上观察到运行超时或内存溢出现象。
    
    \item \textbf{可计算性边界}：DP算法的实际可计算范围严格受限于$n \cdot sum^2$的规模，当两者同时增长时，算法很快到达不可计算区域，凸显了状态空间组合爆炸问题的本质性限制。
\end{itemize}

该实验从实证角度验证了模拟退火算法在大规模输入中具有不可替代的意义——动态规划算法虽然在小规模问题上具有最优性保证，但其计算复杂度使其难以扩展到现实世界的大规模优化问题，这强调了启发式算法在实际应用中的必要性。

\subsection{模拟退火（SA）测试结果}

\subsubsection{base construction}
首先使用 DP 算法验证一批规模较小且均可分解的基准实例，记为 $S_2, S_2, \ldots, S_m$。从中任意选取 $k$ 个作为构造基 (可重复选择)，记为 $A_1, A_2, \ldots, A_k$，并为每个基分配不同的权位，从而构造最终的测试集
\[
  T = \sum_{i=1}^{k} factor_i \times A_i, \ factor_i = random(1, 7^6)
\]
其中“$\times$”表示将集合 $A_i$ 中的每个元素统一乘以权重 $factor_i$。这种分层结构保证了每个 $A_i$ 的可解性在组合后仍然得以保留.

对于每个生成的实例，脚本将数组写入 \texttt{input.txt}，随后最多运行 \(K_{\max} = 10\) 次 \texttt{./sa}，直到其找到正确解为止。每次求解结束后使用 \texttt{./check} 进行正确性验证，并记录首次成功的重启次数。

在实验中, 我们取用 20 组基 (基中最大元素小于300), 每次至少选 10 组来构造 T; 当我们测试了 10000 组解(这样的生成方式很快) 后发现, 若最多允许 \texttt{./sa} 重启 10 次, 我们得到如下结果: 
\begin{itemize}
    \item \textbf{Total instances}: 10{,}000
    \item \textbf{Failed instances}: 73
    \item \textbf{Average first restart count (successful cases)}: 1.10
\end{itemize}

\subsubsection{greedy fill-in}

为了进一步评估模拟退火算法（\texttt{./sa}）的性能与稳定性，我们额外构建了一类 \textbf{保证一定存在可行解} 的测试集。生成脚本通过严格控制总和与元素构造方式，使得每个输入实例都满足三分类问题的可解性条件。

在生成阶段，脚本固定数组长度（本实验中为 \(n = 1000\)），并将所有元素限制在区间 \([1, 10^{6}]\) 内。首先为三个桶分配平衡的目标和，随后逐步生成随机数填充各桶，同时确保不会超出目标和。生成完成后，脚本通过最后一步一致性修正，使得数组总和严格可被桶数整除，从而保证实例一定存在一个 \textbf{完美三分} 方案。

对于每个生成的实例，脚本将数组写入 \texttt{input.txt}，随后最多运行 \(K_{\max} = 10\) 次 \texttt{./sa}，直到其找到正确解为止。每次求解结束后使用 \texttt{./check} 进行正确性验证，并记录首次成功的重启次数。

我们得到如下的结果

\subsubsection{Comparison}
显然 base construction 的构造方式相对 greedy fill-in 而言其实少了很多随机性, 但是 \texttt{./sa} 竟然对 base construction 得到的解有这么好的效果, 这可能提供了一种对三分类问题增加限制条件, 使得该 \texttt{./sa} 能较好解决的思路; 此外, 尽管两者的成功率差距很大, 但是在最大重启次数为 10 时, 其平均首次成功次数竟然都非常接近 1 这说明简单的增加一定规模的重启次数不能有效的增加 SA 的性能

\subsection{综合分析}

\begin{itemize}
    \item DFS+剪枝在小规模数据上效果显著，但在剪枝失效情况下，搜索空间指数增长，性能极差。
    \item DP 算法在中小规模数据上稳定高效，但受限于 $n$ 和 $sum$ 的乘积，超大数据可能无返回。
    \item SA 适用于大规模或近似求解场景，可快速得到可行解，但正确率低于 DP。
\end{itemize}

总体而言，不同算法适合不同规模和场景的三分类问题：DFS 更适合小规模精确求解，DP 适合中规模最优求解，SA 可用于大规模近似求解。

\section{Bonus: 推广到 K-Partition}
我们探讨了当划分份数从 3 份推广到 $K$ 份（$K=4, 5, \dots$）时，即 \textbf{K-Partition Problem}，上述算法的适用性及复杂度变化。

\subsection{深度优先搜索 (DFS): 指数爆炸}
\begin{itemize}
    \item \textbf{适用性}: DFS 算法逻辑有通用性，只需将目标桶数从 3 改为 $K$ 即可直接运行。
    \item \textbf{复杂度分析}: 每个物品现在有 $K$ 种选择。
    \[ T(N, K) = O(K^N) \]
    当 $K$ 增大时，搜索树的宽度（分支因子）迅速增加，计算量呈指数级爆炸。
    \item \textbf{剪枝效果}: 
    虽然基本复杂度变差，但\textit{等效性剪枝 (Symmetry Breaking)} 的效果会随着 $K$ 的增加而变得更加显著。因为初始时 $K$ 个空桶是完全等价的，这使得搜索空间减少了 $K!$ 倍（同构解的数量）。尽管如此，对于较大的 $N$ 和 $K$，DFS 仍难以在有限时间内求解。
\end{itemize}

\subsection{动态规划 (DP): 内存限制}
\begin{itemize}
    \item \textbf{适用性}: 理论上可行，但实际上因内存限制而不可行。
    \item \textbf{状态定义变化}: 为了确定前 $K-1$ 个桶的状态（最后一个桶的状态由总和约束隐含），我们需要维护 $K-1$ 个维度：
    \[ dp[s_1][s_2]\dots[s_{K-1}] \]
    \item \textbf{复杂度分析}:
    \[ \text{时间/空间复杂度} = O(N \cdot \text{Target}^{K-1}) \]
    \item \textbf{实例分析}:
    假设 $\text{Target}=100$，每个维度大小为 100。
    \begin{itemize}
        \item 当 $K=3$ 时，空间为 $100^2 = 10,000$，内存可控。
        \item 当 $K=4$ 时，空间为 $100^3 = 1,000,000$，尚可接受。
        \item 当 $K=10$ 时，空间为 $100^9$，这不仅远超计算机内存物理极限，甚至超过了硬盘存储能力。
    \end{itemize}
    \textbf{结论}: DP 算法不具备向高维推广的能力。
\end{itemize}

\subsection{模拟退火 (Simulated Annealing)}
\begin{itemize}
    \item \textbf{适用性}: 模拟退火在处理 $K$-Partition 问题时表现出极佳的鲁棒性。只需修改目标函数和桶的个数参数，无需改变算法核心逻辑。
    \item \textbf{复杂度分析}:
    \[ T(N, K) \approx O(\text{Iterations} \cdot K) \]
    其复杂度仅随 $K$ 线性增长（主要消耗在能量函数的计算上，若采用增量更新，甚至可以做到 $O(1)$ 与 $K$ 无关）。
    \item \textbf{优势}: 它避开了 DFS 的指数级深度和 DP 的指数级空间，是解决大规模 $K$-Partition 问题（如 $K=100, N=10000$）在工程上可行的方法。
\end{itemize}


\section{结论与讨论 (Conclusion)}
在本项目中，我们深入研究了 3-Partition 问题，并对比了三种不同性质的算法：
\begin{enumerate}
    \item \textbf{DFS} 在强力剪枝的辅助下，是解决中小规模（$N \le 60$）及大数值问题的最佳精确解法。
    \item \textbf{DP} 揭示了该问题的伪多项式性质，适用于 $N$ 较大但数值总和较小的特定场景，但在通用性上受限于内存瓶颈。
    \item \textbf{模拟退火} 作为启发式算法，展示了在处理大规模输入（$N=1000$）和高维变体（Bonus $K$-Partition）时的强大能力，虽然牺牲了完备性，但换取了极高的工程实用性。
\end{enumerate}

\end{document}
