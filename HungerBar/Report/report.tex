\documentclass[13pt,hyperref,a4paper,UTF8]{ctexart}
\usepackage{zjuEnreport}
\usepackage{booktabs}

<<<<<<< HEAD
\documentclass[13pt,hyperref,a4paper,UTF8]{ctexart}
\usepackage{zjuEnreport}
\usepackage{booktabs}

=======
>>>>>>> 9b22acd5f01e375d52ca4b87277e4fc0393b8b80
\renewcommand{\figurename}{Figure}

% 自定义表格前缀（如将“表”改为“Table”）
\renewcommand{\tablename}{Table}
\renewcommand{\contentsname}{Contents}
\renewcommand{\refname}{Reference}
%%-------------------------------正文开始---------------------------%%
\begin{document}

%%-----------------------封面--------------------%%
\cover
\thispagestyle{empty}
%%------------------摘要-------------%%
\newpage
%\begin{abstract}
%\end{abstract}

%%--------------------------目录页------------------------%%
\tableofcontents

%%------------------------正文页从这里开始-------------------%
\newpage

%%可选择这里也放一个标题
\begin{center}
<<<<<<< HEAD
    \title{ \Huge \textbf{{}}}
\end{center}
=======
    \title{ \Huge \textbf{{Three Partition}}}
\end{center}
\vspace{1cm} 
    \textit{\large Abstract: This paper studies the 3-Partition problem, comparing DFS (with pruning), DP and SA algorithms. We test their performance across scales, extend to K-Partition, and find DFS fits small-scale exact solving, DP works for medium cases, and SA handles large-scale scenarios efficiently.}
\vspace{1cm} 

>>>>>>> 9b22acd5f01e375d52ca4b87277e4fc0393b8b80

\section{Project Description}
This project aims to solve the \textbf{3-Partition Problem}. Given a multiset $S = \{a_1, a_2, \dots, a_N\}$ containing $N$ positive integers, we need to determine whether $S$ can be partitioned into three disjoint subsets $S_1, S_2, S_3$ such that the sum of the elements in each subset is equal.

Formally, let the total sum be $Sum = \sum_{i=1}^N a_i$. We need to find a partition that satisfies:
\begin{equation}
    \sum_{x \in S_1} x = \sum_{y \in S_2} y = \sum_{z \in S_3} z = \text{Target}
\end{equation}
where $\text{Target} = Sum / 3$. If $Sum$ is not divisible by 3, it is immediately determined that no solution exists. If such a partition exists, the algorithm must output the specific subset partition scheme; otherwise, it outputs ``no''.

\section{Description of Algorithms}
To address this problem, we designed and implemented three algorithms: Depth-First Search (DFS) with pruning, pseudo-polynomial time Dynamic Programming (DP), and heuristic Simulated Annealing (SA).

\subsection{Algorithm 1: Depth-First Search (DFS) with Pruning}
DFS is an exact algorithm that traverses all possible states by recursively building a search tree. The core of the algorithm is a recursive function \texttt{DFS(index, bucket\_sums)}, where \texttt{index} represents the index of the number currently being processed, and \texttt{bucket\_sums} maintains the cumulative sums of the three buckets. To handle the worst-case exponential search space of $O(3^N)$, we employed two pruning strategies:

\begin{enumerate}
    \item \textbf{Sort Descending}: 
    Before the search begins, we sort the input array $S$ in descending order.
    \begin{itemize}
        \item \textit{Principle}: Prioritizing larger values can reduce the remaining capacity of buckets more quickly. Larger elements have less flexibility (harder to fit into buckets with small remaining space), so they can trigger capacity overflow conditions earlier, achieving ``Fail-Fast'' and significantly reducing the depth of invalid recursion.
    \end{itemize}
    
    \item \textbf{Symmetry Breaking}: 
    This is the core optimization of the algorithm, used to eliminate isomorphic subtrees in the search tree.
    \begin{itemize}
        \item \textit{Principle}: Initially, the three empty buckets are mathematically completely equivalent (i.e., the partition $\{A, B, C\}$ is the same solution as $\{B, A, C\}$). If putting the current number $a_{index}$ into the first empty bucket leads to a subsequent search failure, then attempting to put it into the second or third empty bucket will inevitably yield the same result.
        \item \textit{Implementation}: During the recursion, if the current bucket is empty (Sum=0) and placing the current number into this bucket results in backtracking (meaning this branch has no solution), we strictly stop attempting any subsequent empty buckets. This effectively defines a ``filling order'' for the buckets, reducing the search space by approximately $3!$.
    \end{itemize}
\end{enumerate}

\subsection{Algorithm 2: Dynamic Programming (DP)}
We model this problem as a variant of the multi-dimensional 0/1 Knapsack problem. Since we need to distribute numbers precisely into three buckets, we only need to track the states of the first two buckets; the state of the third bucket is implicitly determined by the law of conservation of the sum.

\begin{itemize}
    \item \textbf{State Definition}: 
    Let $dp[i][j]$ be a boolean value indicating whether it is possible to select a subset of items from the first $k$ items to fill Bucket 1 to capacity $i$ and Bucket 2 to capacity $j$. If $dp[i][j]$ is true, and $TotalSum\_k - i - j \le Target$, then the state is valid.
    
    \item \textbf{State Transition Equation}: 
    For the $k$-th item (with weight $v$), we traverse all possible $(i, j)$ states. The new state $dp_{new}$ can be derived from the previous state $dp_{old}$:
    \begin{equation}
        dp[i][j] = \underbrace{dp[i][j]}_{\text{Put in Bucket 3}} \lor \underbrace{dp[i-v][j]}_{\text{Put in Bucket 1}} \lor \underbrace{dp[i][j-v]}_{\text{Put in Bucket 2}}
    \end{equation}
    The boundary condition is $dp[0][0] = \text{True}$.
    
    \item \textbf{Path Recording and Backtracking}: 
    To output the specific scheme, we maintain a 3D array $path[k][i][j] \in \{1, 2, 3\}$.
    \begin{itemize}
        \item When we place the $k$-th item into Bucket 1 and update state $(i, j)$, we record $path[k][i][j] = 1$.
        \item After the algorithm finishes, if $dp[Target][Target]$ is true, we backtrack from $(N, Target, Target)$ to reconstruct the assignment of each item based on the $path$ array.
    \end{itemize}
\end{itemize}

\subsection{Algorithm 3: Simulated Annealing (SA)}
To solve for large-scale inputs ($N=1000$) or the high-dimensional partition problems in the Bonus section (where Exact Algorithms would fail due to time or memory exhaustion), we implemented Simulated Annealing, a probability-based heuristic algorithm.

\begin{enumerate}
    \item \textbf{Initialization}: 
    Randomly assign $N$ numbers to 3 buckets to form the initial solution $S_0$.
    
    \item \textbf{Cost Function (Energy)}: 
    We define the system's ``energy'' as the degree of imbalance in the current state. The goal is to find the global minimum energy $E=0$.
    \begin{equation}
        E(S) = \sum_{m=1}^{3} |Sum_m - \text{Target}|
    \end{equation}
    
    \item \textbf{Neighbor Generation}: 
    In each iteration, we generate a new state $S'$ by perturbing the current state. Two strategies are adopted:
    \begin{itemize}
        \item \textit{Move}: Randomly select a number and move it from its current bucket to another random bucket.
        \item \textit{Swap}: Randomly select two numbers located in different buckets and swap their positions.
    \end{itemize}
    
    \item \textbf{Metropolis Criterion}: 
    Let $\Delta E = E(S') - E(S)$.
    \begin{itemize}
        \item If $\Delta E < 0$ (the new state is better), accept the new state unconditionally.
        \item If $\Delta E \ge 0$ (the new state is worse), accept the new state with probability $P = \exp(-\Delta E / T)$, where $T$ is the current temperature. This mechanism allows the algorithm to climb ``uphill'' in the early stages to escape local optima.
    \end{itemize}
    
    \item \textbf{Time-Limited Restart}:
    Since SA is a probabilistic algorithm, a single run may not converge. We introduced a restart mechanism: if no solution is found ($E > 0$) when cooling ends and the total runtime has not exceeded 0.9 seconds, we reset the temperature and random seed to start a new round of annealing. This maximizes the probability of finding a solution within a limited time.
\end{enumerate}

\section{Theoretical Analysis of Time Complexity}

\subsection{Problem Complexity Classification}
\begin{itemize}
    \item The \textbf{3-Partition Problem} is a \textbf{Strongly NP-Complete} problem.
    \item Strong NP-completeness implies that the problem remains NP-Complete even if the magnitude (Value) of the input numbers is bounded by a polynomial. This also implies that unless $P=NP$, there is no \textbf{Fully Polynomial Time Approximation Scheme} for this problem.
    \item \textbf{Conclusion}: There is no exact algorithm with polynomial time complexity regarding the bit length of the input data.
\end{itemize}

\subsection{Algorithm 1: Depth-First Search (DFS)}
\begin{itemize}
    \item \textbf{Worst-Case Time Complexity}: $O(3^N)$
    \item \textbf{Detailed Analysis}: 
    The DFS algorithm essentially traverses a ternary tree. For $N$ elements in the set, each element has 3 possible destinations (Bucket 1, Bucket 2, or Bucket 3). Therefore, the size of the unpruned search space is $3 \times 3 \times \dots \times 3 = 3^N$.
    \item \textbf{Impact of Pruning}: 
    Although we implemented \textit{Sort Descending} and \textit{Symmetry Breaking}, which greatly prune the search tree for randomly generated data (making the average branching factor far less than 3), in the worst case (e.g., carefully constructed adversarial data, or when all numbers are equal and there is no solution), pruning may fail, and the algorithm still needs to explore most of the state space. Thus, its theoretical upper bound remains exponential.
\end{itemize}

\subsection{Algorithm 2: Dynamic Programming (DP)}
\begin{itemize}
    \item \textbf{Time Complexity}: $O(N \cdot \text{Target}^2)$, where $\text{Target} = \text{Sum}/3$.
    \item \textbf{Space Complexity}: $O(N \cdot \text{Target}^2)$.
    \item \textbf{Pseudo-polynomial Time}: 
    The formula contains polynomials of $N$ and $\text{Target}$, which seems efficient. However, in computational complexity theory, the time complexity of an algorithm is relative to the input size, which is calculated based on the binary bit length of the data.
    \begin{itemize}
        \item Assuming the maximum value of input numbers is $M$, the number of bits required to input this number is $L = \log_2 M$.
        \item The running time of the algorithm is proportional to the value $M$ (i.e., $\text{Target}$), which means it is proportional to $2^L$.
        \item Therefore, the running time grows \textbf{exponentially} relative to the input size $L$.
    \end{itemize}
    \textbf{Conclusion}: When $N$ is large but the numeric values $M$ are small (e.g., $M \le N$), DP is an effective polynomial-time solution; however, when the numeric values $M$ are large (e.g., $M = 10^{100}$), DP will fail due to memory exhaustion or timeout.
\end{itemize}

\subsection{Algorithm 3: Simulated Annealing (SA)}
\begin{itemize}
    \item \textbf{Time Complexity}: $O(K_{iter} \cdot T_{update})$
    \item \textbf{Analysis}: 
    The complexity of Simulated Annealing is not directly determined by the input size $N$, but by the user-defined cooling schedule.
    \begin{itemize}
        \item $K_{iter}$: Total number of iterations, approximately $\log_{\alpha}(\frac{T_{end}}{T_{start}})$, where $\alpha$ is the cooling coefficient.
        \item $T_{update}$: The cost of each state transition (move or swap) and energy function update. In an optimized implementation, we can complete the energy update in $O(1)$ time via Incremental Update.
    \end{itemize}
    Therefore, the total complexity mainly depends on the preset parameters. In our implementation, we added a hard time limit (0.9 seconds). This makes the algorithm exhibit $O(1)$ constant time complexity in an engineering sense (it always stops within a fixed time relative to infinite problem growth), but at the cost of being unable to guarantee the completeness of the solution or prove the non-existence of a solution.
\end{itemize}

<<<<<<< HEAD
\section{实验与测试分析}

本实验对三种算法方法（DFS+剪枝、动态规划（DP）、模拟退火（SA））在三分类问题上的性能进行了系统测试，测试设计考虑了DFS在小规模输入上的表现, 剪枝策略的优化, 输入规模 $n$ 与总和 $sum$ 的变化对DP算法性能的影响, 以及 SA 算法在大规模输入上的稳定性

\subsection{测试数据设计}

本实验共设计三类测试数据，分别用于评估 DFS、DP 与模拟退火（SA）算法在不同规模与结构下的性能表现。

首先，在 DFS 部分，我们设计了两类测试数据：一类是小规模 $n$ 但具有较大 $sum$ 的输入，用于考察剪枝策略在极端情况下的有效性；另一类是 $n$ 与 $sum$ 均较小的典型输入，用于基准测试 DFS 在常规规模下的运行性能。

对于 DP 算法，我们构造了三组测试数据。第一组固定 $n$ 并逐步增大 $sum$，用于观察 DP 算法在状态空间随总和增大时的时间增长趋势。第二组固定 $sum$ 但逐步增大 $n$，以分析 DP 对输入规模的敏感性。第三组则同时连续增加 $n$ 与 $sum$，用于评估 DP 在大规模数据下的可计算范围，并对其经验时间复杂度进行拟合分析。

最后，在模拟退火（SA）部分，为了构造既能保证可解性、又能使 DP 难以处理的大规模测试实例(符合 SA 的实际使用情况)，我们采用两种思路(base construction 和 greedy fill-in)构造出了n 和sum都较大的测试集, 并让 \texttt{/sa} 在至多重启 10 次的情况下观察性能

\subsection{DFS + 剪枝测试结果}

在小规模 $n$，较大 $sum$ 的数据下，DFS 算法在剪枝策略失效时，计算时间显著增加，部分数据甚至出现计算时间过长的现象。测试结果统计如表~\ref{tab:dfs_time_stats}

\begin{table}
\centering
\caption{DFS 算法运行时间统计（秒）}
\begin{tabular}{c|c|c|c|c}
\hline
$n$ & 平均时间 & 最大时间 & 最小时间 & 方差 \\
=======
\section{Test and Analysis}
\subsection{Design of Test Data}

Three categories of test data were designed in this experiment to evaluate the performance of the DFS, DP, and Simulated Annealing (SA) algorithms under different scales and structures.

First, for the DFS section, We construct a set of test, with a small $n$ but a large $sum$ to examine the effectiveness of the pruning strategy under extreme conditions

For the DP algorithm, three sets of test data were developed. The first set fixed $n$ while gradually increasing $sum$ to observe the time growth trend of the DP algorithm as the state space expands with the total sum. The second set fixed $sum$ while progressively increasing $n$ to analyze the sensitivity of DP to input scale. The third set continuously increased both $n$ and $sum$ simultaneously to evaluate the computable range of DP under large-scale data and perform fitting analysis on its empirical time complexity.

Finally, for the Simulated Annealing (SA) section, to construct large-scale test instances that not only guarantee solvability but also exceed the handling capacity of DP (consistent with the practical application scenarios of SA), we adopted two construction approaches (base construction and greedy fill-in) to generate test sets with large $n$ and $sum$. The performance of \texttt{./sa} was observed with a maximum of 10 restarts allowed.

\subsection{Test Results of DFS + Pruning}

For data with small $n$ and large $sum$, when the pruning strategy failed, the computation time of the DFS algorithm increased significantly, and some data even experienced excessive computation time. The statistical results of the test are shown in Table~\ref{tab:dfs_time_stats}.

\begin{table}
\centering
\caption{Runtime Statistics of DFS Algorithm (Seconds)}
\begin{tabular}{c|c|c|c|c}
\hline
$n$ & Average Time & Maximum Time & Minimum Time & Variance \\
>>>>>>> 9b22acd5f01e375d52ca4b87277e4fc0393b8b80
\hline
10 & 0.0080 & 0.0182 & 0.0054 & 5.12e-05 \\
15 & 0.0081 & 0.0145 & 0.0044 & 2.19e-05 \\
20 & 0.0078 & 0.0149 & 0.0042 & 1.92e-05 \\
25 & 0.0064 & 0.0101 & 0.0043 & 6.82e-06 \\
30 & 0.0921 & 0.1373 & 0.0049 & 0.0045 \\
35 & 2.3341 & 9.2076 & 0.0043 & 7.41 \\
40 & 3.1284 & 11.2849 & 0.0043 & 17.92 \\
<<<<<<< HEAD
45 & --- & 超时 & 0.0040 & --- \\
=======
45 & --- & Timeout & 0.0040 & --- \\
>>>>>>> 9b22acd5f01e375d52ca4b87277e4fc0393b8b80
50 & 103.034 & 254.024 & 0.0044 & 12813.5 \\
\hline
\end{tabular}
\label{tab:dfs_time_stats}
\end{table}

\begin{itemize}
<<<<<<< HEAD
    \item 当剪枝条件失效时，DFS 的搜索空间呈指数级增长，时间复杂度接近 $O(k^n)$，其中 $k$ 为每个元素可选分类数量。
    \item $n=45$ 时出现超时现象，表明剪枝失效对 DFS 的影响极大。
    \item 对于小规模 $n<30$，剪枝有效时算法运行时间在毫秒级，性能较好。
\end{itemize}

\subsection{动态规划（DP）测试结果}

为了系统评估 DP 算法在三分类问题上的性能，我们分别在以下三种典型场景下进行了实验：
（1）固定 $n$、增大 $sum$；（2）固定 $sum$、增大 $n$；（3）同时逐步提高 $n$ 与 $sum$。
以下对三类实验结果分别进行分析。


\subsubsection{固定 $n$，增大 $sum$ 的情况}

在该实验中，我们固定元素数量 $n$，逐步增加总和 $sum$，并记录 DP 算法的运行时间。实验结果如图~\ref{fig:dp_fix_n} 所示。可以明显观察到，随着 $sum$ 的增大，DP 的运行时间呈现非线性增长趋势，但总体与 DP 的理论复杂度 $O(n \cdot sum)$ 保持一致。

\begin{table}[htbp]
\centering
\caption{固定 $n$，增大 $sum$ 时 DP 算法运行时间统计（秒）}
\label{tab:dp_fix_n}
\begin{tabular}{cccccc}
\hline
$total\_sum$ & mean & min & max & var \\
=======
    \item When the pruning condition fails, the search space of DFS grows exponentially, and the time complexity approaches $O(k^n)$, where $k$ is the number of optional partitions for each element.
    \item A timeout occurred when $n=45$, indicating that the failure of pruning has a significant impact on DFS.
    \item For small-scale $n<30$, the algorithm runs in milliseconds when pruning is effective, demonstrating excellent performance.
\end{itemize}

\subsection{Test Results of Dynamic Programming (DP)}

To systematically evaluate the performance of the DP algorithm on the 3-partition problem, experiments were conducted under three typical scenarios:
(1) Fixing $n$ and increasing $sum$; (2) Fixing $sum$ and increasing $n$; (3) Gradually increasing both $n$ and $sum$ simultaneously.
The results of the three types of experiments are analyzed separately below.

\subsubsection{Case 1: Fixing $n$ and Increasing $sum$}

In this experiment, the number of elements $n$ was fixed, the total sum $sum$ was gradually increased, and the runtime of the DP algorithm was recorded. The experimental results are shown in Figure~\ref{fig:dp_fix_n}. It can be clearly observed that as $sum$ increases, the runtime of DP shows a non-linear growth trend, which is generally consistent with the theoretical complexity of DP, i.e., $O(n \cdot sum)$.

\begin{table}[htbp]
\centering
\caption{Runtime Statistics of DP Algorithm When Fixing $n$ and Increasing $sum$ (Seconds)}
\label{tab:dp_fix_n}
\begin{tabular}{cccccc}
\hline
$total\_sum$ & Mean & Min & Max & Var \\
>>>>>>> 9b22acd5f01e375d52ca4b87277e4fc0393b8b80
\hline
10002  & 0.3868 & 0.3765 & 0.4156 & 0.000266 \\
20001  & 1.5310 & 1.4883 & 1.6145 & 0.003158 \\
30000  & 3.3737 & 3.3543 & 3.4249 & 0.000853 \\
40002  & 6.4288 & 6.3932 & 6.4482 & 0.000509 \\
50001  & 11.5226 & 11.3661 & 11.6768 & 0.012789 \\
60000  & 18.0380 & 16.6423 & 23.2785 & 8.589885 \\
70002  & 24.0130 & 22.8261 & 26.7825 & 3.014150 \\
80001  & 30.6717 & 29.9962 & 32.8286 & 1.466136 \\
90000  & 37.7163 & 37.1789 & 39.1276 & 0.661494 \\
100002 & 45.1194 & 43.9791 & 48.8319 & 4.401916 \\
\hline
\end{tabular}
\end{table}

<<<<<<< HEAD
从数据中可以看出：

\begin{itemize}
    \item 当 $sum$ 较小（例如 $10^4$ 以下）时，运行时间增长相对平缓，均值维持在 $O(0.3\sim0.4)$ 秒的水平。
    \item 随着 $sum$ 扩展到 $10^5$ 量级，运行时间呈明显加速增长，尤其是方差也随之增大，显示状态表规模显著增加对性能的影响。
    \item 对不同复杂度假设的拟合结果显示，$O(n^2)$ 拟合曲线与实验数据最为贴合（MSE = 0.772），拟合参数为 $[4.662\times10^{-9}, -0.0684]$，远优于 $O(n)$（MSE = 9.41）或 $O(n \log n)$（MSE = 7.45）的拟合效果。
\end{itemize}

实验数据及拟合曲线见图~\ref{fig:dp_fix_n}。

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figures/fixedn.png} % 将 your_image_file.png 替换为实际图片文件名
\caption{固定 $n$，增大 $sum$ 时 DP 算法运行时间及不同复杂度拟合曲线对比}
\label{fig:dp_fix_n}
\end{figure}

综上，固定 $n$ 条件下，DP 算法运行时间随 $sum$ 单调增加，趋势稳定且可预测；即便在大规模 $sum$ 条件下，其性能仍明显优于 DFS，表现出良好的稳定性和可扩展性。
\subsubsection{固定 $sum$，增大 $n$ 的情况}

本组实验固定总和 $sum$，逐步增加输入规模 $n$，测试结果如图~\ref{fig:dp_fix_sum} 所示。DP 的状态空间大小与 $n$ 成线性关系，因此运行时间的增长主要由迭代次数增加所引起。

\begin{table}[htbp]
\centering
\caption{固定 $sum$，增大 $n$ 时 DP 算法运行时间统计（秒）}
\label{tab:dp_fix_sum}
\begin{tabular}{cccccc}
\hline
$n$ & mean & min & max & var \\
=======
The data reveals the following key observations:

\begin{itemize}
    \item When $sum$ is small (e.g., below $10^4$), the runtime grows relatively steadily, with the mean value maintaining between $0.3$ and $0.4$ seconds.
    \item As $sum$ expands to the order of $10^5$, the runtime increases significantly with a simultaneous rise in variance, indicating the substantial impact of the expanded state table size on performance.
    \item Fitting results for different complexity assumptions show that the $O(n^2)$ fitting curve best aligns with the experimental data (MSE = 0.772) with fitting parameters $[4.662\times10^{-9}, -0.0684]$, outperforming both $O(n)$ (MSE = 9.41) and $O(n \log n)$ (MSE = 7.45) fittings.
\end{itemize}

The experimental data and fitting curves are presented in Figure~\ref{fig:dp_fix_n}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figures/fixedn.png}
\caption{DP Algorithm Runtime and Comparison of Fitting Curves for Different Complexities When Fixing $n$ and Increasing $sum$}
\label{fig:dp_fix_n}
\end{figure}

In summary, with $n$ fixed, the runtime of the DP algorithm increases monotonically with $sum$, showing a stable and predictable trend. Even for large-scale $sum$, its performance is significantly superior to DFS, demonstrating excellent stability and scalability.

\subsubsection{Case 2: Fixing $sum$ and Increasing $n$}

In this set of experiments, the total sum $sum$ was fixed, the input scale $n$ was gradually increased, and the test results are shown in Figure~\ref{fig:dp_fix_sum}. The size of the DP state space is linearly related to $n$, so the increase in runtime is mainly caused by the growing number of iterations.

\begin{table}[htbp]
\centering
\caption{Runtime Statistics of DP Algorithm When Fixing $sum$ and Increasing $n$ (Seconds)}
\label{tab:dp_fix_sum}
\begin{tabular}{cccccc}
\hline
$n$ & Mean & Min & Max & Var \\
>>>>>>> 9b22acd5f01e375d52ca4b87277e4fc0393b8b80
\hline
100  & 4.3085  & 4.2870  & 4.3247  & 0.000170 \\
200  & 10.1568 & 9.9115  & 10.7473 & 0.090331 \\
300  & 19.5679 & 15.8822 & 27.2218 & 16.318133 \\
400  & 22.3316 & 21.8123 & 23.3965 & 0.314617 \\
500  & 30.9510 & 28.7956 & 34.6313 & 4.063425 \\
600  & 36.7893 & 35.6425 & 38.6712 & 1.007763 \\
700  & 47.5941 & 43.3565 & 50.0603 & 5.547414 \\
800  & 56.2361 & 49.5285 & 66.9230 & 38.305580 \\
900  & 62.0149 & 58.8609 & 68.4820 & 12.228341 \\
1000 & 68.6487 & 65.5686 & 76.3955 & 16.920649 \\
\hline
\end{tabular}
\end{table}

<<<<<<< HEAD
拟合曲线~\ref{fig:dp_fix_sum}中可以观察到：

\begin{itemize}
    \item 在中小规模 $n$ 范围内，DP 的运行时间基本保持线性增长，拟合结果表明 $O(n)$ 拟合较为贴合。通过最小二乘法拟合得到的时间复杂度函数为：
=======
As observed from the fitting curve in Figure~\ref{fig:dp_fix_sum}:

\begin{itemize}
    \item Within the range of small and medium-scale $n$, the runtime of DP basically maintains a linear growth trend, and the $O(n)$ fitting provides a good fit. The time complexity function obtained through least squares fitting is:
>>>>>>> 9b22acd5f01e375d52ca4b87277e4fc0393b8b80
    
    \begin{equation}
    T(n) = 0.073154 \cdot n - 4.374624
    \end{equation}
    
<<<<<<< HEAD
    拟合的均方误差（MSE）为 $12.146$ 仍然可以接受
\end{itemize}


\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figures/dp_time_fit_en.png}
\caption{固定 $sum$，增大 $n$ 时 DP 算法运行时间及 O(n) 拟合曲线}
\label{fig:dp_fix_sum}
\end{figure}
\subsubsection{同时增大 $n$ 与 $sum$ 的情况}

为了进一步评估动态规划算法在大规模输入下的极限性能，我们系统地同时增大输入规模 $n$（项目数量）与 $sum$（目标和），使DP的状态空间呈二次方级增长（$O(n \cdot sum^2)$）。测试结果如图~\ref{fig:dp_scale_up}所示。
=======
    The mean squared error (MSE) of the fitting is 12.146, which is still acceptable.
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figures/dp_time_fit_en.png}
\caption{DP Algorithm Runtime and $O(n)$ Fitting Curve When Fixing $sum$ and Increasing $n$}
\label{fig:dp_fix_sum}
\end{figure}

\subsubsection{Case 3: Increasing Both $n$ and $sum$ Simultaneously}

To further evaluate the extreme performance of the dynamic programming algorithm under large-scale inputs, we systematically increased both the input scale $n$ (number of items) and $sum$ (target sum) simultaneously, leading to a quadratic growth of the DP state space ($O(n \cdot sum^2)$). The test results are shown in Figure~\ref{fig:dp_scale_up}.
>>>>>>> 9b22acd5f01e375d52ca4b87277e4fc0393b8b80

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/3D.png}
<<<<<<< HEAD
\caption{动态规划算法在$n$与$sum$同时增大时的性能可视化。图中蓝色散点为实际运行时间，绿色曲面为基于$O(n \cdot sum^2)$复杂度模型的拟合结果。参数拟合结果为$T = 2.07 \times 10^{-10} \cdot n \cdot sum^2 - 0.179$，拟合优度$R^2 = 0.9959$，表明模型能够准确描述算法的时间复杂度。}
\label{fig:dp_scale_up}
\end{figure}

从可视化结果可以清晰观察到，当$n$与$sum$同时扩大时，DP算法的运行时间呈快速增长趋势。具体而言：

\begin{itemize}
    \item \textbf{指数级增长特征}：运行时间随$n$和$sum$的增大呈现明显的非线性增长，与$O(n \cdot sum^2)$的理论复杂度高度一致（$R^2 = 0.9959$）。
    
    \item \textbf{硬件限制触及}：在$n \in [50, 450]$，$sum \in [5208, 47349]$的测试范围内，最大运行时间已显著增长，并在部分数据点上观察到运行超时或内存溢出现象。
    
    \item \textbf{可计算性边界}：DP算法的实际可计算范围严格受限于$n \cdot sum^2$的规模，当两者同时增长时，算法很快到达不可计算区域，凸显了状态空间组合爆炸问题的本质性限制。
\end{itemize}

该实验从实证角度验证了模拟退火算法在大规模输入中具有不可替代的意义——动态规划算法虽然在小规模问题上具有最优性保证，但其计算复杂度使其难以扩展到现实世界的大规模优化问题，这强调了启发式算法在实际应用中的必要性。

\subsection{模拟退火（SA）测试结果}

\subsubsection{base construction}
首先使用 DP 算法验证一批规模较小且均可分解的基准实例，记为 $S_2, S_2, \ldots, S_m$。从中任意选取 $k$ 个作为构造基 (可重复选择)，记为 $A_1, A_2, \ldots, A_k$，并为每个基分配不同的权位，从而构造最终的测试集
\[
  T = \sum_{i=1}^{k} factor_i \times A_i, \ factor_i = random(1, 7^6)
\]
其中“$\times$”表示将集合 $A_i$ 中的每个元素统一乘以权重 $factor_i$。这种分层结构保证了每个 $A_i$ 的可解性在组合后仍然得以保留.

对于每个生成的实例，脚本将数组写入 \texttt{input.txt}，随后最多运行 \(K_{\max} = 10\) 次 \texttt{./sa}，直到其找到正确解为止。每次求解结束后使用 \texttt{./check} 进行正确性验证，并记录首次成功的重启次数。

在实验中, 我们取用 20 组基 (基中最大元素小于300), 每次至少选 10 组来构造 T; 当我们测试了 10000 组解(这样的生成方式很快) 后发现, 若最多允许 \texttt{./sa} 重启 10 次, 我们得到如下结果: 
=======
\caption{Performance Visualization of Dynamic Programming Algorithm When Both $n$ and $sum$ Increase Simultaneously. The blue scatter points represent the actual runtime, and the green surface is the fitting result based on the $O(n \cdot sum^2)$ complexity model. The parameter fitting result is $T = 2.07 \times 10^{-10} \cdot n \cdot sum^2 - 0.179$ with a goodness of fit $R^2 = 0.9959$, indicating that the model can accurately describe the time complexity of the algorithm.}
\label{fig:dp_scale_up}
\end{figure}

The visualization results clearly show that as both $n$ and $sum$ expand, the runtime of the DP algorithm grows rapidly. Specifically:

\begin{itemize}
    \item \textbf{Exponential Growth Characteristic}: The runtime exhibits a distinct non-linear growth trend with the increase of $n$ and $sum$, which is highly consistent with the theoretical complexity of $O(n \cdot sum^2)$ ($R^2 = 0.9959$).
    
    \item \textbf{Hardware Limitation Reached}: Within the test range of $n \in [50, 450]$ and $sum \in [5208, 47349]$, the maximum runtime has increased significantly, and timeout or memory overflow phenomena have been observed at some data points.
    
    \item \textbf{Computability Boundary}: The actual computable range of the DP algorithm is strictly limited by the scale of $n \cdot sum^2$. When both parameters grow simultaneously, the algorithm quickly enters the non-computable region, highlighting the inherent limitation of the state space combinatorial explosion problem.
\end{itemize}

This experiment empirically verifies the irreplaceable significance of the simulated annealing algorithm for large-scale inputs. Although the dynamic programming algorithm guarantees optimality for small-scale problems, its computational complexity prevents it from scaling to real-world large-scale optimization problems, emphasizing the necessity of heuristic algorithms in practical applications.

\subsection{Test Results of Simulated Annealing (SA)}

\subsubsection{Base Construction}

First, a set of small-scale, solvable benchmark instances was verified using the DP algorithm, denoted as $S_1, S_2, \ldots, S_m$. An arbitrary selection of $k$ instances (with repetition allowed) was made from these as construction bases, denoted as $A_1, A_2, \ldots, A_k$. Each base was assigned a distinct weight to construct the final test set:
\[
  T = \sum_{i=1}^{k} factor_i \times A_i, \ factor_i = random(1, 7^6)
\]
where the symbol "$\times$" denotes multiplying each element in set $A_i$ by the weight $factor_i$. This hierarchical structure ensures that the solvability of each individual $A_i$ is preserved after combination.

For each generated instance, the script writes the array to \texttt{input.txt} and then runs \texttt{./sa} a maximum of $K_{\max} = 10$ times until a correct solution is found. After each solution attempt, \texttt{./check} is used for correctness verification, and the number of restarts required for the first successful solution is recorded.

In the experiment, 20 base instances were used (with the maximum element in each base less than 300), and at least 10 bases were selected each time to construct $T$. After testing 10,000 instances (a process that is computationally efficient with this generation method), the following results were obtained when allowing a maximum of 10 restarts for \texttt{./sa}:

>>>>>>> 9b22acd5f01e375d52ca4b87277e4fc0393b8b80
\begin{itemize}
    \item \textbf{Total instances}: 10{,}000
    \item \textbf{Failed instances}: 73
    \item \textbf{Average first restart count (successful cases)}: 1.10
\end{itemize}

<<<<<<< HEAD
\subsubsection{greedy fill-in}

为了进一步评估模拟退火算法（\texttt{./sa}）的性能与稳定性，我们额外构建了一类 \textbf{保证一定存在可行解} 的测试集。生成脚本通过严格控制总和与元素构造方式，使得每个输入实例都满足三分类问题的可解性条件。

在生成阶段，脚本固定数组长度（本实验中为 \(n = 1000\)），并将所有元素限制在区间 \([1, 10^{6}]\) 内。首先为三个桶分配平衡的目标和，随后逐步生成随机数填充各桶，同时确保不会超出目标和。生成完成后，脚本通过最后一步一致性修正，使得数组总和严格可被桶数整除，从而保证实例一定存在一个 \textbf{完美三分} 方案。

对于每个生成的实例，脚本将数组写入 \texttt{input.txt}，随后最多运行 \(K_{\max} = 10\) 次 \texttt{./sa}，直到其找到正确解为止。每次求解结束后使用 \texttt{./check} 进行正确性验证，并记录首次成功的重启次数。

我们得到如下的结果

\subsubsection{Comparison}
显然 base construction 的构造方式相对 greedy fill-in 而言其实少了很多随机性, 但是 \texttt{./sa} 竟然对 base construction 得到的解有这么好的效果, 这可能提供了一种对三分类问题增加限制条件, 使得该 \texttt{./sa} 能较好解决的思路; 此外, 尽管两者的成功率差距很大, 但是在最大重启次数为 10 时, 其平均首次成功次数竟然都非常接近 1 这说明简单的增加一定规模的重启次数不能有效的增加 SA 的性能

\subsection{综合分析}

\begin{itemize}
    \item DFS+剪枝在小规模数据上效果显著，但在剪枝失效情况下，搜索空间指数增长，性能极差。
    \item DP 算法在中小规模数据上稳定高效，但受限于 $n$ 和 $sum$ 的乘积，超大数据可能无返回。
    \item SA 适用于大规模或近似求解场景，可快速得到可行解，但正确率低于 DP。
\end{itemize}

总体而言，不同算法适合不同规模和场景的三分类问题：DFS 更适合小规模精确求解，DP 适合中规模最优求解，SA 可用于大规模近似求解。

\section{Bonus: 推广到 K-Partition}
我们探讨了当划分份数从 3 份推广到 $K$ 份（$K=4, 5, \dots$）时，即 \textbf{K-Partition Problem}，上述算法的适用性及复杂度变化。

\subsection{深度优先搜索 (DFS): 指数爆炸}
\begin{itemize}
    \item \textbf{适用性}: DFS 算法逻辑有通用性，只需将目标桶数从 3 改为 $K$ 即可直接运行。
    \item \textbf{复杂度分析}: 每个物品现在有 $K$ 种选择。
    \[ T(N, K) = O(K^N) \]
    当 $K$ 增大时，搜索树的宽度（分支因子）迅速增加，计算量呈指数级爆炸。
    \item \textbf{剪枝效果}: 
    虽然基本复杂度变差，但\textit{等效性剪枝 (Symmetry Breaking)} 的效果会随着 $K$ 的增加而变得更加显著。因为初始时 $K$ 个空桶是完全等价的，这使得搜索空间减少了 $K!$ 倍（同构解的数量）。尽管如此，对于较大的 $N$ 和 $K$，DFS 仍难以在有限时间内求解。
\end{itemize}

\subsection{动态规划 (DP): 内存限制}
\begin{itemize}
    \item \textbf{适用性}: 理论上可行，但实际上因内存限制而不可行。
    \item \textbf{状态定义变化}: 为了确定前 $K-1$ 个桶的状态（最后一个桶的状态由总和约束隐含），我们需要维护 $K-1$ 个维度：
    \[ dp[s_1][s_2]\dots[s_{K-1}] \]
    \item \textbf{复杂度分析}:
    \[ \text{时间/空间复杂度} = O(N \cdot \text{Target}^{K-1}) \]
    \item \textbf{实例分析}:
    假设 $\text{Target}=100$，每个维度大小为 100。
    \begin{itemize}
        \item 当 $K=3$ 时，空间为 $100^2 = 10,000$，内存可控。
        \item 当 $K=4$ 时，空间为 $100^3 = 1,000,000$，尚可接受。
        \item 当 $K=10$ 时，空间为 $100^9$，这不仅远超计算机内存物理极限，甚至超过了硬盘存储能力。
    \end{itemize}
    \textbf{结论}: DP 算法不具备向高维推广的能力。
\end{itemize}

\subsection{模拟退火 (Simulated Annealing)}
\begin{itemize}
    \item \textbf{适用性}: 模拟退火在处理 $K$-Partition 问题时表现出极佳的鲁棒性。只需修改目标函数和桶的个数参数，无需改变算法核心逻辑。
    \item \textbf{复杂度分析}:
    \[ T(N, K) \approx O(\text{Iterations} \cdot K) \]
    其复杂度仅随 $K$ 线性增长（主要消耗在能量函数的计算上，若采用增量更新，甚至可以做到 $O(1)$ 与 $K$ 无关）。
    \item \textbf{优势}: 它避开了 DFS 的指数级深度和 DP 的指数级空间，是解决大规模 $K$-Partition 问题（如 $K=100, N=10000$）在工程上可行的方法。
\end{itemize}


\section{结论与讨论 (Conclusion)}
在本项目中，我们深入研究了 3-Partition 问题，并对比了三种不同性质的算法：
\begin{enumerate}
    \item \textbf{DFS} 在强力剪枝的辅助下，是解决中小规模（$N \le 60$）及大数值问题的最佳精确解法。
    \item \textbf{DP} 揭示了该问题的伪多项式性质，适用于 $N$ 较大但数值总和较小的特定场景，但在通用性上受限于内存瓶颈。
    \item \textbf{模拟退火} 作为启发式算法，展示了在处理大规模输入（$N=1000$）和高维变体（Bonus $K$-Partition）时的强大能力，虽然牺牲了完备性，但换取了极高的工程实用性。
=======
\subsubsection{Greedy Fill-in}

Given the exceptional performance of \texttt{./sa} observed with the first construction approach, an additional set of test instances \textbf{guaranteed to have feasible solutions} was constructed to further evaluate the stability of the simulated annealing algorithm (\texttt{./sa}). The generation script strictly controls the total sum and element construction method to ensure that each input instance satisfies the solvability conditions of the 3-partition problem.

During the generation phase, the script fixes the array length (set to $n = 1000$ in this experiment) and restricts all elements to the interval $[1, 10^{6}]$. First, balanced target sums are assigned to the three partitions. Random numbers are then sequentially generated to fill each partition while ensuring no exceedance of the target sum. After generation, a final consistency correction step is performed to ensure the total sum of the array is exactly divisible by the number of partitions, thus guaranteeing the existence of a \textbf{perfect 3-partition} solution for each instance.

For each generated instance, the script writes the array to \texttt{input.txt} and then runs \texttt{./sa} a maximum of $K_{\max} = 10$ times until a correct solution is found. After each solution attempt, \texttt{./check} is used for correctness verification, and the number of restarts required for the first successful solution is recorded.

The obtained results are as follows:

\begin{itemize}
    \item \textbf{Total instances}: 1000
    \item \textbf{Failed instances}: 534
    \item \textbf{Average first restart count (successful cases)}: 1.18
\end{itemize}

\subsubsection{Comparison}

Evidently, the base construction approach introduces significantly less randomness compared to the greedy fill-in method, yet \texttt{./sa} achieves remarkably better performance on instances generated via base construction. This insight suggests a potential research direction: imposing additional constraints on the 3-partition problem to form new subproblems that can be efficiently solved by the proposed \texttt{./sa} algorithm. Furthermore, despite the substantial gap in success rates between the two approaches, the average number of restarts for the first successful solution is very close to 1 when the maximum number of restarts is set to 10. This indicates that simply increasing the number of restarts within a certain range does not effectively improve the performance of SA.

\subsection{Comprehensive Analysis}

\begin{itemize}
    \item DFS + Pruning achieves excellent performance on small-scale data, but its performance degrades drastically when pruning fails due to exponential growth of the search space.
    \item The DP algorithm is stable and efficient for small-to-medium-scale data, but it may fail to return results for extremely large data due to constraints imposed by the product of $n$ and $sum$.
    \item SA is suitable for large-scale or approximate solution scenarios, enabling rapid acquisition of feasible solutions, although with a lower accuracy rate compared to DP.
\end{itemize}

In conclusion, different algorithms are suited for 3-partition problems of varying scales and scenarios: DFS is more suitable for small-scale exact solutions, DP for medium-scale optimal solutions, and SA for large-scale approximate solutions.

\section{Bonus: Generalization to K-Partition}

We explore the applicability and complexity changes of the aforementioned algorithms when the number of partitions is generalized from 3 to $K$ ($K=4, 5, \dots$), i.e., the \textbf{K-Partition Problem}.

\subsection{Depth-First Search (DFS): Exponential Explosion}
\begin{itemize}
    \item \textbf{Applicability}: The DFS algorithm exhibits logical generality and can be directly applied by simply modifying the number of target partitions from 3 to $K$.
    \item \textbf{Complexity Analysis}: Each item now has $K$ possible choices.
    \[ T(N, K) = O(K^N) \]
    As $K$ increases, the width (branching factor) of the search tree expands rapidly, leading to an exponential explosion in computational complexity.
    \item \textbf{Pruning Effect}: 
    Although the baseline complexity deteriorates, the effectiveness of \textit{Symmetry Breaking Pruning} becomes more pronounced with the increase of $K$. Since the initial $K$ empty partitions are completely equivalent, the search space is reduced by a factor of $K!$ (the number of isomorphic solutions). Nevertheless, DFS remains impractical for large $N$ and $K$ within a reasonable time frame.
\end{itemize}

\subsection{Dynamic Programming (DP): Memory Limitation}
\begin{itemize}
    \item \textbf{Applicability}: Theoretically feasible, but practically infeasible due to severe memory constraints.
    \item \textbf{State Definition Change}: To determine the state of the first $K-1$ partitions (the state of the last partition is implicitly constrained by the total sum), a $K-1$-dimensional state space must be maintained:
    \[ dp[s_1][s_2]\dots[s_{K-1}] \]
    \item \textbf{Complexity Analysis}:
    \[ \text{Time/Space Complexity} = O(N \cdot \text{Target}^{K-1}) \]
    \item \textbf{Case Analysis}:
    Assume $\text{Target}=100$, with each dimension size set to 100.
    \begin{itemize}
        \item For $K=3$, the space complexity is $100^2 = 10,000$, which is memory-controllable.
        \item For $K=4$, the space complexity is $100^3 = 1,000,000$, which is still acceptable.
        \item For $K=10$, the space complexity is $100^9$, which far exceeds the physical memory limits of computers and even surpasses the capacity of hard disk storage.
    \end{itemize}
    \textbf{Conclusion}: The DP algorithm lacks the ability to generalize to high-dimensional partition problems.
\end{itemize}

\subsection{Simulated Annealing (SA)}
\begin{itemize}
    \item \textbf{Applicability}: Simulated annealing demonstrates excellent robustness in handling the K-Partition problem. Only the objective function and the number of partitions need to be modified without changing the core logic of the algorithm.
    \item \textbf{Complexity Analysis}:
    \[ T(N, K) \approx O(\text{Iterations} \cdot K) \]
    Its complexity grows only linearly with $K$ (primarily attributed to the computation of the energy function; with incremental updates, this can even be optimized to $O(1)$, independent of $K$).
    \item \textbf{Advantages}: It avoids the exponential depth of DFS and the exponential space requirement of DP, making it an engineering-feasible solution for large-scale K-Partition problems (e.g., $K=100, N=10000$).
\end{itemize}

\section{Conclusion and Discussion}

In this project, we conducted an in-depth study of the 3-Partition problem and compared three algorithms with distinct characteristics:
\begin{enumerate}
    \item \textbf{DFS}, augmented with powerful pruning techniques, is the optimal exact solution method for small-to-medium-scale problems ($N \le 60$) and problems involving large numerical values.
    \item \textbf{DP} reveals the pseudo-polynomial nature of the problem and is suitable for specific scenarios with large $N$ but small total sums. However, its general applicability is constrained by memory bottlenecks.
    \item \textbf{Simulated Annealing}, as a heuristic algorithm, demonstrates exceptional capabilities in handling large-scale inputs ($N=1000$) and high-dimensional variants (the Bonus K-Partition problem). While sacrificing completeness, it achieves remarkable engineering practicality.
>>>>>>> 9b22acd5f01e375d52ca4b87277e4fc0393b8b80
\end{enumerate}

\end{document}
